%++++++++++++++++++++++++++++++++++++++++
% Don't modify this section unless you know what you're doing!
\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath,amssymb,amsthm}  % improve math presentation
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\myleqa{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily (a)}}}{\leq}}}
\newcommand\myleqb{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily (b)}}}{\leq}}}
\newcommand\myleqc{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily (c)}}}{\leq}}}
\newcommand\myeqa{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily (a)}}}{=}}}
\newcommand\myeqb{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily (b)}}}{=}}}
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=black,        % color of internal links
	citecolor=black,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
\usepackage{setspace}
\doublespacing
%++++++++++++++++++++++++++++++++++++++++

\begin{document}

\title{
{\includegraphics[width=0.7\columnwidth]{university.jpg}}\\
{Greedy Principal Flows}\\
{\large National University of Singapore}\\
}
\author{Sebastian Lie}
\date{05 March 2021}
\maketitle

\chapter*{Acknowledgements}
My 7 soft toys. And my desk lamp for being the light of my life.

\chapter*{Abstract}
Principal Flows are a great tool to use when we want to extend the notion of Principal Component analysis to multivariate datasets that we know lie on non-linear manifolds. We restrict this problem to constructing principal flows on hyperspheres. We use a different, easier method to obtain the principal flow that is even closer to its canonical PCA interpretation. 

\newpage
\tableofcontents
\newpage

\chapter*{Introduction}

\section{Motivation}

For principal flows or motivation for coding algorithm?

With the advent of Big Data, using machine learning on multivariate datasets to solve problems from disparate fields has grown in popularity.
Unsupervised learning has also grown in popularity, Principal Component Analysis being the most popular algorithm. However, if it is given data that lie on some manifold, or a non-linear space, PCA fails to achieve good results. This is why we want to use Principal flows: in essence they are a generalisation of PCA to manifolds. Furthermore, in keeping with the trends of the data science field, we want to use a popular language, python to construct these principal flows.

\section{Background and Definitions}

We first define notation and some important concepts we will use later.
We will work throughout with a 

\newpage

\subsection{Definitions}

\begin{table}[]
\begin{tabular}{|l|l|}
\hline
\textbf{Notation} & \textbf{Explanation}                                      \\ \hline
$\mathbb{R}^D$    & D dimensional space of real numbers.                      \\ \hline
\textbf{X}        & Data Matrix, D dimensions                                 \\ \hline
D                 & The original dimension of the data matrix                 \\ \hline
d                 & Dimension of lower dimensional data which X is reduced to \\ \hline
M                 & Denote a Manifold embedded in $\mathbb{R}^d$              \\ \hline
$p$               & Denotes a point on the manifold, M.                       \\ \hline
\textbf{v}        & Denotes a vector                                          \\ \hline
$T_pM$            & Denotes the Tangent space of a point p on the Manifold.   \\ \hline
\textbf{S}        & Denotes a Covariance matrix                               \\ \hline
\{x\_1,...x\_n\}  & Denotes a collection of points on the manifold, M.        \\ \hline
\end{tabular}
\end{table}

Now we define the notion of vector fields.

\subsection{Vector Fields}

Contextualise vector fields in terms of the eigenvector field: $\textbf{X}(p) = (p, \epsilon(p))$.

\begin{itemize}
    \item[] A vector \textit{at point p}, $p \in \mathbb{R}^{n+1}$ is a pair $\mathbf{v = (p, v)}, \ v \in \mathbb{R}^{n+1}$, such that $\mathbf{v}$ is the vector $v$ translated so that its tail is at $p$ instead of the origin. All vector operations are defined such that the first item of the pair remains the same, and the second item is the result of the operation. The length and angle between 2 vectors are the same as normal vectors rooted in the origin.
    \item[] \textbf{Definition:} A \textit{vector field} \textbf{X} on $U \subset \mathbb{R}^{n+1}$ is a function which assigns to each point
    of U a vector at that point. $$\textbf{X}(p) = (p, X(p))$$ for some function $X: U \longrightarrow \mathbb{R}^{n+1}$. Vector fields on $\mathbb{R}^{n+1}$ are often most easily described by specifying this associated function X.
    \item[] Take for example, the eigenvector field: $\textbf{X}(p) = (p, \epsilon(p))$ 
    \begin{figure}[h]
        \begin{center}
        	\includegraphics[scale=0.8]{fig2.5.PNG}
    		\caption{An example of a vector field, $\mathbb{R}^3$}
    		\label{fig:Graph 4}
        \end{center}
    \end{figure}
\end{itemize}
\newpage

\subsection{Graphs and Level Sets}

\begin{itemize}
    \item[] \textbf{Definition}: Given a function $f: U \longleftarrow \mathbb{R}$, where $U \subset \mathbb{R}^{n+1}$. it's level sets $f^{-1}(c)$ are defined, for each real number c, by $$f^{-1}(c) = \{ (x_1, ....., x_{n+1}) \in U: f(x_1,....x_{n+1}) = c \}$$ i.e all the input values for f that result in a value of c.
    \item[] Then c is known as the \textit{height} of the level set and $f^{-1}(c)$ is the level set at height c, or the solution set of the equation $f(x_1,....x_{n+1}) = c$.
    \item[] \textbf{Definition}: The \textit{graph} of a function $f: U \longleftarrow \mathbb{R}$, is the subset of $\mathbb{R}^{n+2}$ defined by $$graph(f) = \{ (x_1,...x_{n+2}) \in \mathbb{R}^{n+2}: (x_1,...x_{n+1}) \in U \ and \ x_{n+2} = f(x_1,....x_{n+1}) \}$$
    \item[] For $c \geq 0$ the level set of f at height c is just the set of all points in the domain of f over which the graph is at distance c. Graphs are simply the illustration of the function where one of the axes is the output of f itself, and the other axes are its inputs. Fixing the $x_{n+2}$ value of the graph gives a level set, as illustrated below.
    \item[] For n = 1, level sets are (at least for non-constant differentiable functions) generally curves in $\mathbb{R}^2$. These curves play the same roles as \textbf{contour lines} on a topographic map. Just as contour maps provide an accurate picture of the topography of a land, so does a knowledge of the level sets and their heights accurately portray the graph of a function.
    \item[] Often it is hard to \textbf{visualise a graph} in higher dimensions, and thus we use level sets to help us do so. The level set of a graph of f with 3 dimensional input can be visualised in this way: let $x_i$ = constant, and the plane formed will cut the level set $f^{-1}(c)$ (c fixed) in some subset, usually a curve. Letting the plane move, by changing the selected value of the xi-coordinate, these subsets will generate the level set $f^{-1}(c)$. This is illustrated below.
\end{itemize}

\newpage

\subsection{Logarithm Maps}
\textbf{Logarithm Map:} For each $p \in M$, let
$$log_p(x): M \longrightarrow T_pM$$
be the logarithm map. The log map is a function that projects a point $x_i$ on the manifold M onto the plane tangent to M at p, by producing a vector which indicates the direction in which p should move to obtain the projection of $x \in M$ onto $T_pM$.\\
\\
\textbf{Exponential Map:} For each $p \in M$, let
$$exp_p(x): T_pM \longrightarrow M$$
be the exponential map. Exponential maps are the inverse of the logarithm maps.
Given a point p, the exponential map w.r.t M, a manifold, is a function that projects a point from the plane tangent to M at p onto $x$ on the manifold M.\\

\subsection{Tangent Space}
Contextualise tangent space as the space at $T_pM$.
\begin{itemize}
    \item[] For a smooth function f, a vector u is said to be \textit{tangent to the level set} $f^{-1}(c)$ if it is a velocity vector of a parametrized curve in $\mathbb{R}^{n+1}$ whose image is contained in $f^{-1}(c)$.
\end{itemize}

\subsection{Geodesics}

Geodesics extend the concept of straight lines in the Euclidean space to Manifolds. A geodesic is the straight line distance between $x_i$ and $x_j$ on some manifold M.

\subsection{Eigenvalues and Eigenvectors}
Now we define eigenvalues and eigenvectors
\textbf{Definition}: Let $\textbf{A} \in \mathbb{R}^{n \times n}$. Then a non-zero vector \textbf{v} is an eigenvector of \textbf{A} if there exists some scalar $\lambda$ such that $\textbf{A}\textbf{v} = \lambda \textbf{v}$. Then $\lambda$ is known as the eigenvalue corresponding to vector \textbf{v}.\\
\\
Here we also note that for any 2 eigenvectors $v_i$ and $v_j$, $v_i \cdot v_j = 0$, or any 2 eigenvectors are orthogonal to each other.

\subsection{Diagonalisation}




\chapter*{Literature Review}

should be 4 pages long

\section{Linear Dimension Reduction}

\textbf{Linear Dimensionality Reduction\footnote{This definition from \href{https://jmlr.csail.mit.edu/papers/volume16/cunningham15a/cunningham15a.pdf}{here}.}:} \textit{Given $n$ D-dimensional data points: $\textbf{X}= [x_1,....x_n]^T \in \mathbb{R}^{n \times D}$ and a choice of dimensionality $d < D$, optimise some objective function $f_X(\Dot{})$ to produce a linear transformation $P \in \mathbb{R}^{p \times r}$ and call $Y = XP \in \mathbb{R}^{n \times d}$}

\subsection{Principal Component Analysis}

\textbf{Principal Component analysis} is a linear dimension reduction method that tries to obtain a lower-dimensional representation of the data that retains as much variation as possible present in the data set. PCA does this by constructing principal components (PCs) which are linear combinations of the original variables. These PCs are uncorrelated (orthogonal) and ordered in descending order by the amount of variation retained. Then reducing some high-dimensional data of dimension $D$ to a lower dimensional $d$ is simply a matter of computing the first $d$ principal components.\\
\textbf{Algorithm}
\begin{itemize}
    \item 1. First we center the data matrix, \textbf{X}, by 
    \item 2. Next we compute the covariance matrix of the centered data matrix.
    \item 3. Now we diagonalise the covariance matrix and take the first d eigenvectors. Let these d eigenvectors be the columns of a matrix $\textbf{V}_d$.
    \item 4. Multiply \textbf{X} by $\textbf{V}_d$ to obtain the d principal components.
\end{itemize}

\subsection{Classical MDS, Euclidean Distance}

\textbf{Multidimensional Scaling} is a linear dimensionality reduction method. Its main aim is to take some high dimensional data, \textbf{X} and find some low dimensional points so as to minimize the discrepancy between the pairwise distances in the original space and the pairwise distances in the lower-dimensional space. \\
Let a squared dissimilarity matrix between n points be $\textbf{M} \in \mathbb{R}^{n \times n}$, let the matrix of coordinates be denoted by $\textbf{X} \in \mathbb{R}^{n \times D}$, and let $\textbf{B} = \textbf{X}\textbf{X}'$. Since dissimilarities do not change under translations, we assume that \textbf{X} has column means equal to 0. MDS seeks to find a lower dimensional representation $\textbf{X}_{(d)} \in \mathbb{R}^{n \times d}$.\\
Algorithm: from 2005 Book Modern Multidimensional Scaling, pg 262
\begin{itemize}
    \item 1. Compute or obtain the dissimilarity matrix, \textbf{D}.
    \item 2. Let \textbf{J} be the centering matrix: $\textbf{J} = \textbf{I} - n^{-1}\mathbf{1}\mathbf{1}'$. Compute $\textbf{B} = -\frac{1}{2}\textbf{J}\textbf{D}\textbf{J}$.
    \item 3. Then, compute the eigendecomposition of \textbf{B}: $\textbf{B} = \textbf{V}\mathbf{\Lambda}\textbf{V}'$. 
    \item 4. Then $\textbf{X} = \mathbf{\Lambda}^{1/2}\textbf{V}'$ and for a q-dimensional representation  of \textbf{X}: $\textbf{X}_{(p)}$, $\textbf{X}_{(p)} = \mathbf{\Lambda}^{1/2}_p\textbf{V'}_p$, where $\mathbf{\Lambda}^{1/2}_p$ is the first $p \times p$ submatrix of $\Lambda$, and $\textbf{V'}_p$ is the first p columns of \textbf{V}', i.e the first q eigenvectors and their corresponding eigenvalues.
\end{itemize}
Note that using Euclidean distances, the result of MDS is the same as PCA.
Advantages of MDS over PCA: If we have $n << p$, then MDS is likely to be more efficient, since MDS finds eigenvectors of a $n \times n$ matrix and a $p \times p$ matrix respectively. (From 2002 book on Principal Component Analysis)
MDS simple to implement, and their optimizations do not involve local minima despite their inherent limitations as linear methods.

\section{Non-Linear Dimension Reduction}

Non-linear dimensionality reduction methods are particularly useful when the multivariate data we obtain is sampled from a smooth non-linear manifold, e.g a manifold in an S-shape, obtains better estimates than linear methods like PCA and MDS, which implicitly assume that the data is sampled from a linear space. Certain data sets contain essential nonlinear structures that are invisible to PCA and MDS.

\newpage

\subsection{Principal Geodesic Analysis}

Principal Geodesic Analysis(PGA) is a generalization of principal component analysis to connected, complete manifolds. The main aim of PGA is to be able to describe the variability of some data lying on some manifold M. More formally, the goal is to find a sequence of lower-dimensional subspaces, which on manifolds are nested geodesic submanifolds that maximize the projected variance of the data. These submanifolds are called the principal geodesic submanifolds, which are analogous to linear subspaces for PCA.\\
\\
Let $T_pM$ be the tangent space of M at the intrinsic mean $p$ of the $x_i$. The intrinsic mean of some data \textbf{X} lying on some manifold is obtained by first setting the initial mean to a random data point, then iteratively obtaining a better estimation of the intrinsic mean by computing the average of the vectors obtained using the log map at the current mean on all data points, then taking the next estimate of the intrinsic mean as the projection of that average using the exponential map at the current estimate.\\
Let $U \subset T_pM$ be a neighbourhood of 0 in which our data, $\{x_1,...x_n\}$ lies, such that projection is well defined for all geodesic submanifolds of $exp_p(U)$.These principal geodesic submanifolds are obtained by by first constructing an orthonormal basis of tangent vectors $v_1,...,v_d \in T_pM$ that span the tangent space. These vectors are then used to form the principal geodesic subspaces $H_k$ where $H_k = exp_p(V_k)$, and $V_k$ is intersection of the subspace spanned by vectors $1..k$ and $U$.
subspace $V_k = span(\{v_1,...,v_k\})\bigcap U$.\\
\textbf{Algorithm}:
\begin{itemize}
    \item 1. Obtain the intrinsic mean, $p \in M$ of $\{x_1,..x_n\}$.
    \item 2. Calculate the vectors $u_i = log_p(x_i)$.
    \item 3. Calculate the covariance matrix $\textbf{S} = \frac{1}{n} \sum^n_{i=1} u_iu_i^T$.
    \item 4. Diagonalise the covariance matrix to obtain $\{v_k, \lambda_k\}$ the eigenvectors and eigenvalues respectively, which represent the principal directions in the tangent space $T_pM$ and the variances.
\end{itemize}

\newpage

\subsection{Locally Linear Embedding}

\textbf{Locally Linear Embedding} 
\\
The main idea of LLE is to construct a neighbourhood preserving mapping of the original points of dimension $D$ to the reconstructed points of dimension $d$. We assume that for some data $\textbf{X} \in \mathbb{R}^{D}$, the data lie on or near a smooth manifold of dimensionality $d << D$. Locally, LLE assumes the embedding is linear, and thus for each data point $p \in \mathbb{R}^D$, we aim to use a linear combination of its K nearest neighbours to reconstruct a lower-dimensional $p \in \mathbb{R}^d$. LLE does this by first learning some reconstruction weights from the D-dimensional data: $\textbf{W}$ where $\textbf{W}_{ij}$ represents the contribution of the j-th data point in reconstructing the i-th one. These weights obey an important symmetry: for any particular data point, they are invariant to rotations, rescalings, and translations of that data point and its neighbors. They thus reflect intrinsic geometric properties of the  It then uses the same weights $\textbf{W}_{ij}$ that reconstruct the ith data point in $D$ dimensions to also reconstruct its embedded manifold coordinates in $d$ dimensions. (Should I explain why LLE does this?) At the end of LLE, each  D-dimensional observation $\textbf{X}_i$ is mapped to a low dimensional vector $\textbf{Y}_i$ representing global internal coordinates on the manifold.

Embedding Cost Function:
$$\Phi(\textbf{Y}) = \sum_i |\textbf{Y}_i - \sum_j \textbf{W}_{ij}\textbf{Y}_j|^2$$

\textbf{LLE Algorithm:}
\begin{itemize}
    \item 1. Compute the K nearest Neighbors of each original data point $\textbf{X}_i$, where K is a hyperparameter.
    \item 2. Compute the weights $\textbf{W}_{ij}$ that best reconstruct each data point from it's neighbours minimizing the \textbf{Reconstruction Error} below by constrained linear fits.
$$\varepsilon (\textbf{W}) = \sum_i|\textbf{X}_i - \sum_j \textbf{W}_{ij} \textbf{X}_j|^2$$
    \item 3. Compute the vectors $\textbf{Y}_i$ best reconstructed by the weights $\textbf{W}_{ij}$ by minimising the quadratic form in eq (2) by its bottom $d$ non-zero eigenvectors.
\end{itemize}

\newpage

\subsection*{Isomap}

Isomap is an approach that combines the major algorithmic features of PCA and MDS — computational efficiency, global optimality, and asymptotic convergence guarantees—with the flexibility to learn a broad class of nonlinear manifolds. It can be thought of as an extension of MDS to manifolds. Isomap achieves this by estimating the geodesic distance between data points, given only input-space distances, e.g euclidean distance between points.\\
\\
This relies on the fact that for neighboring points, input-space distance provides a good approximation to geodesic distance. For faraway points, Isomap approximates geodesic distance by adding up a sequence of “short hops” between neighboring points, computed efficiently by finding shortest paths in a graph with edges connecting neighboring data points.  This approximation relies on the proof that for a sufficiently high density (a) of data points, we can always choose a neighborhood size (e or K) large enough that the graph will (with high probability) have a path not much longer than the true geodesic, but small enough to prevent edges that “short circuit” the true geometry of the manifold.\\
\textbf{Isomap Algorithm}:
\begin{itemize}
    \item 1. Construct neighbourhood graph $G$: First, we need to compute the distances between points: for any node i and j, connect the 2 nodes if $d(i,j) < \epsilon$ or if j is one of the k-nearest neighbours of i.
    \item 2. Compute all-pairs shortest paths on $G$. There are many algorithms to do this, but we use the Floyd-Warshall algorithm.
    \item 3. Construct d-dimensional embedding, using MDS.
\end{itemize}

\chapter*{Greedy Principal Flows}

\section{Problem Statement}

The main objective for greedy principal flows was to quantify or describe multivariate data on the manifold: we cannot simply fit a line, as this is not Euclidean space. Instead, we want some curve or path such that, locally, it follows the path of maximal variation of the data. This is done by taking the largest eigenvector from the diagonalisation of the covariance matrix. This can be thought of as the manifold extension of the euclidean 1st principal component.

Problem statement: 

\section{An explanation of the algorithm}

Algorithm explanation: 
Assume for now that our data, \textbf{X} lies on the hypersphere. We know that the first principal component passes through the centroid of the data. We want our principal flow to do the same. Thus, we first need to find the centroid of the data. Start at an arbitrary data point, p. Then we project all other data points onto $T_pM$, then calculate the covariance matrix of the points on $T_pM$ and find the first eigenvector from diagonalisation, and move in that direction. 

Then we start from that centroid, and build our principal flow from there.

\section{Algorithm Steps}

Defn: The Principal Flow of the dataset is basically an integral curve that is always tangent to the direction of 'maximal variation' at any given point it contains.
\begin{itemize}
    \item Computes the principal flow of the dataset. 
    Idea: This is a "greedy" implementation of the principal flow 
    algorithm, developed originally by Professor Yao Zhi Gang.
    
    \item We assume the underlying structure of the data is a hypersphere.
    Starting from the centroid of the data set (user defined or calculated below), we apply the following procedure: 
    \item 1. Project the data residing on the hypersphere onto the hyperplane
    tangent to the centroid of the data (p). We use the log map of p, 
    obtaining a matrix of vectors on the tangent plane that point from p
    to the projected points. These are the plane vectors.

    \item 2. Compute the largest principal component of the points using the plane vectors, applying weights as necessary via the kernel function provided.

    \item 3. The largest principal component is the new directions that the principal flow moves in. We determine it's sign by making sure it is moving in the same direction as the previous principal direction.

    \item 4. We take a small step in each direction on the plane,
    then project it back to the hypersphere.

    \item 5. We do 1-4 for the point on the opposite end of the growing principal flow.

    \item 6. Store both points. 
    
    \item 7. Repeat 1-6 until max\_iter is reached.
\end{itemize}

\chapter*{Principal Boundary}

\begin{itemize}
    \item Computes the principal flow of the dataset. 
    Idea: This is a "greedy" implmentation of the principal flow 
    algorithm, developed originally by Professor Yao Zhi Gang.
    
    \item We assume the underlying structure of the data is a hypersphere.
    Starting from the centroid of the data set (user defined or calculated below), we apply the following procedure: 
    \item 1. Project the data residing on the hypersphere onto the hyperplane
    tangent to the centroid of the data (p). We use the log map of p, 
    obtaining a matrix of vectors on the tangent plane that point from p
    to the projected points. These are the plane vectors.

    \item 2. Compute the largest principal component of the points using the plane vectors, applying weights as necessary via the kernel function provided.

    \item 3. The largest principal component is the new directions that the principal flow moves in. We determine it's sign by making sure it is moving in the same direction as the previous principal direction.

    \item 4. We take a small step in each direction on the plane,
    then project it back to the hypersphere.

    \item 5. We do 1-4 for the point on the opposite end of the growing principal flow.

    \item 6. Store both points. 
    
    \item 7. Repeat 1-6 until max\_iter is reached.
\end{itemize}

\section{Algorithm}

\chapter*{Applications}
Talk about how data is generated
\subsection{Toy Data}

First we generate some toy data

\includegraphics[]{Data_13.png}

With the principal flow in red.

\includegraphics[]{single_flow_13.png}

We can see that the principal flow accurately reconstructs the toy data, the s curve on the sphere.

\subsection{Noisy Toy Data}

\subsection{Real World Data}


\chapter*{Future Directions}

What open questions can we investigate further?


\chapter*{References}
\begin{thebibliography}{9}


\bibitem{mds}
Ingwer Borg and Patrick J.F. Groenen (2005) \textit{Modern Multidimensional Scaling}, Springer.

\bibitem{pga}
P. Thomas Fletcher, Conglin Lu, Stephen M. Pizer and Sarang Joshi \textit{Principal Geodesic Analysis for the Study of
Nonlinear Statistics of Shape}

\bibitem{isomap}



\end{thebibliography}

\end{document}